{
  "weights_path": "Absolute path to the YOLO pose model (.pt, .onnx, etc.) you want to run. Choose the file exported from training.",
  "save_dir": "Folder where SimBA will write the tracked pose CSV files. Leave blank to keep results in memory only.",
  "bp_config_csv_path": "CSV listing body-part names in the order expected by the model. Each name on its own line (e.g., Nose, LeftEar, ...).",
  "batch_dropdown": "Number of frames processed at once. Larger batches speed up inference but require more GPU RAM.",
  "verbose_dropdown": "Toggle console printouts for progress and timings. Keep TRUE while tuning, FALSE for quiet runs.",
  "workers_dropdown": "How many CPU worker threads to use for pre/post-processing. Set <= available cores.",
  "format_dropdown": "Export/serialization format. Training: export the trained model to this format after training (None = PyTorch .pt only). Inference: match the format of your weights file, or None to auto-detect from file extension. Options: onnx, engine, torchscript, onnxsimplify, coreml, openvino, pb, tf, tflite, torch.",
  "img_size_dropdown": "Resize shorter image side to this many pixels before inference. Larger sizes improve accuracy but slow down processing.",
  "devices_dropdown": "Compute device to run on. Select CUDA device ID for GPU or CPU.",
  "interpolate_dropdown": "Fill missing detections by interpolating coordinates over time. Recommended for cleaner trajectories.",
  "stream_dropdown": "Enable streaming inference which reads frames sequentially (lower memory footprint). Disable for random seeking.",
  "threshold_dropdown": "Minimum detection confidence to keep. Lower values detect more poses but introduce more false positives.",
  "iou_dropdown": "Minimum bounding-box overlap required to keep the same track between frames. Higher values demand tighter alignment.",
  "max_tracks_dropdown": "Maximum number of simultaneous object tracks the model can follow in each frame.",
  "max_track_per_id_dropdown": "Cap on how many concurrent tracks a single class ID (e.g., a specific animal) can occupy.",
  "smoothing_dropdown": "Apply temporal smoothing over this many milliseconds to reduce jitter in keypoint trajectories.",
  "recursive_dropdown": "If False, only analyses videos in the immediate parent folder. If True, searches all sub-directories as well.",
  "CPU_ROI_DESCRIPTIVE_ANALYSIS": "If few videos (<1000), consider single CPU. If more videos (>1000), consider higher CPU count for acceptable speed.",
  "CPU_TIMEBINS_MOVEMENT": "If the total number of time bins over all videos is expected to be many (1000+), consider higher CPU count (>1) for acceptable speed.",
  "SHOW_ANIMAL_BBOX": "Display rectangle bounding boxes encompassing all animal keypoints",
  "BORDER_BG_COLOR": "The color of the border image on which data is printed",
  "USE_GPU": "If GPU available and selected, then potentially faster processing times.",
  "OUPUT_VIDEO_QUALITY": "Higher values produce larger, higher quality, videos. Lower values produce smaller, lower quality videos. \nScale not linear. Avoid values above 80 if possible. 60 generally works well.",
  "OUTPUT_VIDEO_QUALITY": "Higher values produce larger, higher quality, videos. Lower values produce smaller, lower quality videos. \nScale not linear. Avoid values above 80 if possible. 60 generally works well.",
  "VIDEO_SPEED": "2 doubles the speed, while 0.5 halves the speed.",
  "VIDEO_CODEC": "Video codec to use for encoding. Common options include libx264 (CPU, widely compatible), h264_nvenc (NVIDIA GPU), libx265 (CPU, better compression), and libvpx-vp9 (CPU, web optimized). Codec availability depends on your FFmpeg installation.",
  "RESOLUTION_CONCAT": "If the two videos have different resolutions, which resolution should be retained in each video of the concatenated results?",
  "LOCATION_FRAME_COUNT": "The location in the video where the \n frame count is positioned.",
  "ROTATE_FILL_COLOR": "When video is rotated, there may be empty space not \n covered by the video. What color should this space have?",
  "VIDEO_DIR": "Directory containing videos.",
  "SAVE_DIR": "Directory where to save results.",
  "CONCAT_HEIGHT": "If join involves aligning videos horizontally, this values \nwill be used to ensure videos have the same height. ",
  "CONCAT_WIDTH": "If join involves aligning videos vertically, this values \nwill be used to ensure videos have the same width. ",
  "CONCAT_RES_HEADER": "When stacking videos horizontally and/or vertically, \n the videos need to be same height and/or width. Here, \n select what resolution to use.",
  "EGOCENTRIC_DATA_DIR": "Folder containing pose-estimation CSV data.\n Can be sub-directory in 'project_folder/csv' folder.\n Should contain same file names as the VIDEO files",
  "EGOCENTRIC_VIDEO_DIR": "Folder containing videos.\n Should contain same file names as the DATA files.",
  "EGOCENTRIC_ANCHOR": "This body-part will be placed in the center of the video",
  "EGOCENTRIC_DIRECTION_ANCHOR": "This body-part will be placed at N degrees relative to the anchor",
  "EGOCENTRIC_DIRECTION": "The anchor body-part will always be placed at these degrees relative to the center anchor",
  "CORE_COUNT": "Higher core counts speeds up processing but may require more RAM memory",
  "KLEINBERG_SIGMA": "Higher values (e.g., 2-3) produce fewer but longer bursts; lower values (e.g., 1.1-1.5) detect more frequent, shorter bursts. Must be > 1.01",
  "KLEINBERG_GAMMA": "Higher values (e.g., 0.5-1.0) reduce total burst count by making downward transitions costly; lower values (e.g., 0.1-0.3) allow more flexible state changes",
  "KLEINBERG_HIERARCHY": "Hierarchy level to extract bursts from (0=lowest, higher=more selective).\n Level 0 captures all bursts; level 1-2 typically filters noise; level 3+ selects only the most prominent, sustained bursts.\nHigher levels yield fewer but more confident detections",
  "KLEINBERG_HIERARCHY_SEARCH": "If True, searches for target hierarchy level within detected burst periods,\n falling back to lower levels if target not found. If False, extracts only bursts at the exact specified hierarchy level.\n Recommended when target hierarchy may be sparse.",
  "KLEINBERG_SAVE_ORIGINALS": "If True, saves the original data in a new sub-directory of \nthe project_folder/csv/machine_results directory",
  "yolo_map_path": "Path to the YOLO dataset YAML file. Defines class names, paths to train/val images and labels, and number of keypoints.",
  "yolo_initial_weights_path": "Optional path to pretrained weights (.pt) to start training from (e.g. yolo11n-pose.pt). Leave blank to train from scratch.",
  "epochs_dropdown": "Number of training epochs. More epochs can improve accuracy but increase overfitting risk and training time.",
  "plots_dropdown": "If TRUE, generate and save training curves (loss, mAP, etc.) in the save directory.",
  "patience_dropdown": "Early-stopping patience: training stops if validation metric does not improve for this many epochs.",
  "simba2yolo_config": "Path to the SimBA project configuration file (.ini). Defines project paths, body-parts, and animals.",
  "simba2yolo_train_size": "Percentage of sampled frames to use for the YOLO training set. The remainder is used for validation. E.g. 70 means 70% train, 30% val.",
  "simba2yolo_padding": "Extra margin (as a fraction of image size) added around the keypoint bounding box. Use a small value (e.g. 0.05–0.2) if the tight box cuts off body parts or you want more context in each crop; None or 0 = no padding.",
  "simba2yolo_sample_size": "Maximum number of frames to sample per video for creating YOLO images and labels. Higher values give more data but increase processing time.",
  "simba2yolo_grey": "If TRUE, extracted video frames are saved in greyscale. If FALSE, frames are saved in color.",
  "simba2yolo_clahe": "If TRUE, apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to frames before saving. Can improve keypoint visibility in low-contrast videos.",
  "yolo_plot_line_thickness": "Thickness of the lines drawn between keypoints (skeleton). AUTO lets the plotter choose based on video size; or set 1–20 pixels.",
  "yolo_plot_circle_size": "Radius of the circles drawn at each keypoint. AUTO lets the plotter choose based on video size; or set 1–20 pixels.",
  "yolo_plot_tracks": "If TRUE, draw trajectory paths (tracks) for each detected instance over time. If FALSE, draw only keypoints and skeleton per frame.",
  "yolo_plot_data_path": "Path to a single YOLO pose result CSV (output from YOLO pose inference). Must match the video you select.",
  "yolo_plot_video_path": "Path to the video file to overlay pose results onto. Filename should match the data CSV (without extension).",
  "yolo_plot_data_dir": "Directory containing YOLO pose result CSV files. Used for batch plotting; each CSV is matched to a video of the same name in the video directory.",
  "SLEAP_DATA_DIR": "Directory containing SLEAP CSV prediction files. Each CSV should match a video filename (without extension).",
  "ANIMAL_COUNT": "Number of animals (tracks) in the videos. Used to name classes (e.g. animal_1, animal_2) in the YOLO dataset.",
  "sleap_remove_animal_ids": "If TRUE, merge all tracks into a single identity (animal_1). Use when animal IDs are not meaningful or for single-animal data.",
  "sleap_threshold": "Minimum SLEAP instance confidence (the instance.score column in the CSV). Only pose predictions with score ≥ this value are used when building the YOLO dataset. E.g. 90 means keep instances where instance.score ≥ 0.9; lower values include more frames but may add noisy predictions.",
  "SLEAP_SLP_DATA_DIR": "Directory containing SLEAP .SLP project/annotation files. Each .SLP file is converted to YOLO pose format.",
  "BATCH_CLIP_START_TIME": "Start time for clipping all videos. Use HH:MM:SS (e.g. 00:00:10). Click APPLY to copy this value to every video row.",
  "BATCH_CLIP_END_TIME": "End time for clipping all videos. Use HH:MM:SS (e.g. 00:01:30). Must be after start time. Click APPLY to copy this value to every video row.",
  "BATCH_DOWNSAMPLE_WIDTH": "Output width in pixels when downsampling all videos. Must be even. Click APPLY to copy to every video row.",
  "BATCH_DOWNSAMPLE_HEIGHT": "Output height in pixels when downsampling all videos. Must be even. Click APPLY to copy to every video row.",
  "BATCH_FPS": "Target frames per second for all videos. Click APPLY to copy this value to every video row.",
  "BATCH_APPLY_DOWNSAMPLE": "Tick to check the downsample box for all videos (all will be downsampled on execute). Untick to uncheck all. Set width and height in the DOWNSAMPLE VIDEOS panel, then click APPLY there to copy those values to every row.",
  "BATCH_APPLY_FPS": "Tick to enable FPS change for all videos.\nUntick to disable for all.\nSet FPS in CHANGE FPS panel, then APPLY there.",
  "BATCH_APPLY_GREYSCALE": "Tick to enable greyscale for all videos.\nUntick to disable for all.",
  "BATCH_APPLY_FRAME_COUNT": "Tick to superimpose frame numbers on all videos.\nUntick to disable for all.",
  "BATCH_APPLY_CLAHE": "Tick to apply CLAHE (contrast) to all videos.\nUntick to disable for all.",
  "BATCH_APPLY_CLIP": "Tick to enable clipping for all videos.\nUntick to disable for all.\nSet start/end in CLIP VIDEOS SETTING, then APPLY there."
}