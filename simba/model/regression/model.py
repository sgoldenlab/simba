from typing import Optional, Dict, List
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import StratifiedKFold
from simba.utils.checks import check_valid_array, check_int, check_valid_lst, check_str, check_float, check_instance, check_valid_dataframe
from simba.utils.enums import Formats
from simba.utils.errors import DataHeaderError
from simba.model.regression.metrics import (mean_absolute_percentage_error,
                                            mean_squared_error,
                                            mean_absolute_error,
                                            r2_score,
                                            root_mean_squared_error)

def fit_xgb(x: pd.DataFrame,
            y: np.ndarray,
            objective: Optional[str] = 'reg:squarederror',
            n_estimators: Optional[int] = 100,
            max_depth: Optional[int] = 6,
            verbosity: Optional[int] = 1,
            learning_rate: Optional[float] = 0.3,
            tree_method: Optional[str] = 'auto'):
    """
    :example:
    >>> x = pd.DataFrame(np.random.randint(0, 500, (100, 20)))
    >>> y = np.random.randint(1, 6, (100,))
    >>> mdl = fit_xgb(x=x, y=y)
    """
    OBJECTIVES = ('reg:squarederror', 'reg:squaredlogerror', 'reg:logistic', 'reg:pseudohubererror')
    TREE_METHODS = ('auto', 'exact', 'approx', 'hist', 'gpu_hist')

    check_valid_dataframe(df=x, source=f'{fit_xgb.__name__} x', valid_dtypes=Formats.NUMERIC_DTYPES.value)
    check_valid_array(data=y, source=f'{fit_xgb.__name__} y', accepted_ndims=(1,), accepted_axis_0_shape=[x.shape[0]], accepted_dtypes=Formats.NUMERIC_DTYPES.value)
    check_str(name=f'{fit_xgb.__name__} objective', value=objective, options=OBJECTIVES)
    check_str(name=f'{fit_xgb.__name__} tree_method', value=tree_method, options=TREE_METHODS)
    check_int(name=f'{fit_xgb.__name__} n_estimators', value=n_estimators, min_value=1)
    check_int(name=f'{fit_xgb.__name__} max_depth', value=max_depth, min_value=1)
    check_int(name=f'{fit_xgb.__name__} verbosity', value=verbosity, min_value=0, max_value=3)
    check_float(name=f'{fit_xgb.__name__} learning_rate', value=learning_rate, min_value=0.1, max_value=1.0)
    xgb_reg = xgb.XGBRegressor(objective=objective, max_depth=max_depth, n_estimators=n_estimators, verbosity=verbosity)
    return xgb_reg.fit(X=x, y=y)

def transform_xgb(x: pd.DataFrame, model: xgb.XGBRegressor):

    """
    :example:
    >>> x, y = pd.DataFrame(np.random.randint(0, 500, (100, 20))), np.random.randint(1, 6, (100,))
    >>> mdl = fit_xgb(x=x, y=y)
    >>> new_x = pd.DataFrame(np.random.randint(0, 500, (100, 20)))
    >>> results = transform_xgb(x=new_x, model=mdl)
    """

    check_instance(source=transform_xgb.__name__, instance=model, accepted_types=(xgb.XGBRegressor,))
    check_valid_dataframe(df=x, source=f'{transform_xgb.__name__} x', valid_dtypes=Formats.NUMERIC_DTYPES.value)
    expected_x_names = model.get_booster().feature_names
    new_x_names = [str(i) for i in list(x.columns)]
    missing_x_names = set([i for i in expected_x_names if i not in new_x_names])
    additional_x_names = set([i for i in new_x_names if i not in expected_x_names])
    if len(additional_x_names) > 0:
        raise DataHeaderError(msg=f'The new data has {len(additional_x_names)} features not expected by the model: {additional_x_names}', source=transform_xgb.__name__)
    if len(missing_x_names) > 0:
        raise DataHeaderError(msg=f'The new data are missing {len(missing_x_names)} features expected by the model: {missing_x_names}', source=transform_xgb.__name__)
    if expected_x_names != new_x_names:
        raise DataHeaderError(msg=f'The new data contains features in the wrong order from the expected features', source=transform_xgb.__name__)
    return np.round(model.predict(x), 2)


def evaluate_xgb(y_pred: np.ndarray,
                 y_true: np.ndarray,
                 metrics: List[str],
                 stratified: Optional[bool] = False) -> dict:
    """
    Evaluates the performance of a regression model (e.g., XGBoost) by calculating selected metrics. Optionally, the evaluation can be stratified by unique
    values in the true target variable (`y_true`), where performance is computed separately for each class/level.

    :param np.ndarray y_pred: Predicted values generated by the model, must have the same shape as `y_true`.
    :param np.ndarray y_true: True target values to compare the predictions against.
    :param List[str] metrics: List of metrics to compute.
    :param stratified: If True, computes the metric for each unique class/level in `y_true`. If False (default), computes the metric for the entire dataset.
    :return: A dictionary containing the computed metrics.
    :rtype: dict

    :example:
    >>> x = pd.DataFrame(np.random.randint(0, 500, (100, 20)))
    >>> y = np.random.randint(1, 6, (100,))
    >>> mdl = fit_xgb(x=x, y=y)
    >>> new_x = pd.DataFrame(np.random.randint(0, 500, (100, 20)))
    >>> y_pred = transform_xgb(x=new_x, model=mdl)
    >>> evaluate_xgb(y_pred=y_pred, y_true=y, metrics=['MAE', 'MAPE', 'RMSE', 'MSE'])
    """
    METRICS = {'MAPE': mean_absolute_percentage_error, 'MSE': mean_squared_error, 'MAE': mean_absolute_error, 'R2': r2_score, 'RMSE': root_mean_squared_error}
    check_valid_array(data=y_true, source=evaluate_xgb.__name__, accepted_ndims=(1,), accepted_dtypes=Formats.NUMERIC_DTYPES.value)
    check_valid_array(data=y_pred, source=evaluate_xgb.__name__, accepted_ndims=(1,), min_axis_0=y_true.shape[0],accepted_dtypes=Formats.NUMERIC_DTYPES.value)
    check_valid_lst(data=metrics, source=f'{evaluate_xgb.__name__} metrics', valid_values=list(METRICS.keys()))
    results = {}
    for metric in metrics:
        if not stratified:
            results[metric] = METRICS[metric](y_true=y_true, y_pred=y_pred)
        else:
            results[metric] = {}
            for unique_true in (np.unique(y_true)):
                sample_idx = np.argwhere(y_true == unique_true)
                sample_y_true, sample_y_pred = y_true[sample_idx].flatten(), y_pred[sample_idx].flatten()
                results[metric][unique_true] = METRICS[metric](y_true=sample_y_true, y_pred=sample_y_pred)

    return results


def kfold_fit_xgb(x: pd.DataFrame,
                  y: np.ndarray,
                  objective: Optional[str] = 'reg:squarederror',
                  n_estimators: Optional[int] = 100,
                  max_depth: Optional[int] = 6,
                  verbosity: Optional[int] = 1,
                  learning_rate: Optional[float] = 0.3,
                  tree_method: Optional[str] = 'auto',
                  k: Optional[int] = 5):

    # USE xgb.cv
    #
    # OBJECTIVES = ('reg:squarederror', 'reg:squaredlogerror', 'reg:logistic', 'reg:pseudohubererror')
    # TREE_METHODS = ('auto', 'exact', 'approx', 'hist', 'gpu_hist')
    #
    # check_valid_dataframe(df=x, source=f'{fit_xgb.__name__} x', valid_dtypes=Formats.NUMERIC_DTYPES.value)
    # check_valid_array(data=y, source=f'{fit_xgb.__name__} y', accepted_ndims=(1,), accepted_axis_0_shape=[x.shape[0]], accepted_dtypes=Formats.NUMERIC_DTYPES.value)
    # check_str(name=f'{fit_xgb.__name__} objective', value=objective, options=OBJECTIVES)
    # check_str(name=f'{fit_xgb.__name__} tree_method', value=tree_method, options=TREE_METHODS)
    # check_int(name=f'{fit_xgb.__name__} n_estimators', value=n_estimators, min_value=1)
    # check_int(name=f'{fit_xgb.__name__} max_depth', value=max_depth, min_value=1)
    # check_int(name=f'{fit_xgb.__name__} verbosity', value=verbosity, min_value=0, max_value=3)
    # check_float(name=f'{fit_xgb.__name__} learning_rate', value=learning_rate, min_value=0.1, max_value=1.0)
    # check_int(name=f'{fit_xgb.__name__} k', value=k, min_value=2)
    # k_fold = StratifiedKFold(n_splits=k, shuffle=True)
    # for fold_cnt, (train_index, test_index) in enumerate(k_fold.split(x, y)):
    #     x_fold, y_fold = x.loc[train_index], y[test_index]

    pass


# x = pd.DataFrame(np.random.randint(0, 500, (100, 20)))
# y = np.random.randint(1, 6, (100,))
# mdl = fit_xgb(x=x, y=y)
# new_x = pd.DataFrame(np.random.randint(0, 500, (100, 20)))
# y_pred = transform_xgb(x=new_x, model=mdl)
# evaluate_xgb(y_pred=y_pred, y_true=y, metrics=['MAE', 'MAPE', 'RMSE', 'MSE'], stratified=True)
#


# x = pd.DataFrame(np.random.randint(0, 500, (100, 20)))
# y = np.random.randint(1, 6, (100,))
# #mdl = fit_xgb(x=x, y=y)
#
# kfold_fit_xgb(x=x, y=y)

    #for fold_cnt, (train_index, test_index) in enumerate(k_fold.split(x_data, y_data)):